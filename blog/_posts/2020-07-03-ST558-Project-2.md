---
title: "Modeling Populairty of Web Articles"
layout: post
subtitle: Data Science project for NCSU ST-558
bigim: /blog/addons/2020-07-03-ST558-Project-2/correlation_hm.png
--- 

Project 2 of my summer semester class, Data Science for Statisticians, was quite the fun project. The project comes at the close of studying Non-Linear Ensemble methods and Generalized Linear models (including simple, multiple, and general linear models). We were challenged to draw upon this recent material as well as RMarkdown, tidyverse, and general R programming topics introduced at the start of the semester. The work involved allowed me to gain a deeper insight into the mechanics of principle component analysis (PCA), cross validation, and ensemble non-linear model methods. 

Specifically, the challenge was to use a data set with 57 predictor variables pertaining to the contents, structure, and attributes of online articles to predict the number of shares an article would get. The data set came from the work done by researchers at the University of Portugal. Their approach, from which I drew inspiration from, was to turn the number of shares into a binary classification problem. If the article received more than 1,400 shares is was considered **popular** and otherwise **unpopular**. Therefore, the challenge for me became a classification problem. Additionally, I had the liberty of only generating a model that had predictive power and not one that was interpretable.

My approach can be read in detail [here](https://github.com/9Olive/DS4S/tree/master/Projects/P2), but I will outline my process in this post. After exploring and preparing the data set I set out to use PCA to reduce the dimensionality. My personal challenge was the day-and-a-half I had available to work with the data. (Taking 3 summer classes is a pressure cooker for outputting assignments). Ideally, I would like to have spent much more time becoming familiar with the context of the data. Several of the metrics were calculated using natural language processing frameworks. I believe that the set of predictors were a treasure trove of latent structure that could have been sussed out with feature engineering tactics. Using PCA allowed me to remain ignorant of much of the context brought by the predictors. That said, it did not come without its own set of challenges. 

PCA or singular value decomposition is an extremely was extremely useful, and I look forward to employing it more. It was difficult to asses the number of components to keep. Initially, I read about the Kaiser rule (the default in SPSS), which is to simply drop all components with an eigenvalue less than 1. Ultimately my reading led me to [Horn's method](https://files.eric.ed.gov/fulltext/EJ1101205.pdf). That method, named after psychologist [John Horn](https://en.wikipedia.org/wiki/John_L._Horn), involves bootstrap sampling the dataset and recomputing and comparing the PCA eigenvalues from the simulated data to the actual data's PCA eigenvalues. 

I can appreciate the complexity and art form that is modeling. There is not a paved path nor steadfast rule to guide an statistician or data scientist. There are hints here and there, but the cost comes at being meticulous and diligent while investigating a data set. More over, the value of having subject matter knowledge is immense. My biggest take-away from the project was learning how to automate RMarkdowns and modeling. I was able to produce 7 reports, 14 models leveraging cross validation, and only with the press of a "button". 
