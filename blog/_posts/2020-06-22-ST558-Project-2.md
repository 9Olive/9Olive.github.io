---
title: "Modeling Populairty of Web Articles"
layout: post
subtitle: Data Science project for NCSU ST-558
--- 

Project 2 of my summer semester class, Data Science for Statisticians was quite the fun project. The project comes at the close of studying Non-Linear Ensemble methods and Generalized Linear models (include simple, multiple, and general linear models). We were challenged to draw upon this recent material as well as RMarkdown topics introduced at the start of the semester some three to four weeks ago. The work involved allowed me to gain a deeper insight into the mechanics of principle component analysis (PCA), cross validation, and ensemble non-linear model methods. 

Specifically, the challenge was to use a data set with 57 predictor variables pertaining to the contents, structure, and attributes of online articles to predict the number of shares an article would get. The data set came from the work done by researchers at the University of Portugal. Their approach, from which I drew elements of mine from, was to turn the number of shares into a binary classification problem. If the article received more than 1,400 shares is was considered **popular** and otherwise **unpopular**. Therefore, the challenge for me became a classification problem. Additionally, I had the liberty of only generating a model that had predictive power and not necessarily interpretability.

My approach can be read in detail [here](https://github.com/9Olive/DS4S/tree/master/Projects/P2), but I will outline my process in this post. After exploring and preparing the data set I set out to use PCA to reduce the dimensionality. My personal challenge was the day-and-a-half I had available to work with the data. (Taking 3 summer classes is a pressure cooker for work). Ideally, I would like to have spent much more time becoming familiar with the context of the data. Several of the metrics that demonstrated explained the variance were calculated using natural language processing frameworks. I believe that the set of predictors were a treasure trove of latent structure that could have been sussed out with feature engineering tactics. Using PCA allowed me to remain ignorant of much of the context the predictors wider data set had, but it did not come without its own challenges. 

PCA or singular value decomposition is an extremely useful tool to keep in the belt. It was difficult to asses the number of components to keep. Initially, I read about the Kaiser rule (the default in SPSS), which is to simple drop all components with an eigenvalue less than 1. Ultimately my reading led me [Horn's method](https://files.eric.ed.gov/fulltext/EJ1101205.pdf). That method, named after psychologist [John Horn](https://en.wikipedia.org/wiki/John_L._Horn), involves bootstrap sampling the dataset and recomputing and comparing the PCA eigenvalues from the simulated data to the actual data's PCA eigenvalues. 

I can appreciate the complexity and art form that is modeling. There is not a paved path nor steadfast rule to guide an statistician or data scientist. There are hints here and there, but the cost comes at being meticulous and diligent while investigating a data set. More over, the value of having subject matter knowledge is immense. My biggest take-away from the project was learning how to automate RMarkdowns and modeling. I was able to produce 7 reports, 14 models, all with cross validation with the press of a button. 

*"People can have the Model T in any color - so long as it's black." *
- Henry Ford