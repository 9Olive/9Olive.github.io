# then store the squared residual
err[i] <- (iris$Petal.Length[i] - predict(lm, newdata = iris[i,]))**2
}
# report the average squared residual
print(round(mean(err), 4))
some_data <- data.frame(predictor = 1:5, response = LETTERS[1:5])
print(some_data)
for (i in 1:nrow(some_data)) {
for (j in 2:nrow(some_data))
if (i < j & sum(i,j) < 7) {
print(some_data[c(i,j),])
}
}
exhaustive_cv <- data.frame(records = 5:100) %>%
mutate(models = factorial(records) / (2 * factorial(records - 2)))
exhaustive_cv %>%
ggplot() +
geom_line(aes(x = records, y = models)) +
labs(x = '# of Observations in Original Data Set',
y = '# of Models Generated for Cross-Validation',
title = 'Cross-Validation Models Generated per Data Set Size')
large_p <- expand.grid(records = 5:100, p = 2:5) %>%
mutate(models = factorial(records) / (2 * factorial(records - 2)))
large_p <- as.matrix(large_p)
plot_ly(data = large_p, z = ~models) %>% add_surface()
large_p <- expand.grid(records = 5:100, p = 2:5) %>%
mutate(models = factorial(records) / (2 * factorial(records - 2)))
large_p <- as.matrix(large_p)
large_p <- expand.grid(records = 5:100, p = 2:5) %>%
mutate(models = factorial(records) / (2 * factorial(records - 2)))
plot_ly(x = large_p$records, y = large_p$p, z = large_p$models) %>% add_surface()
large_p <- as.matrix(large_p)
plot_ly(x = large_p$records, y = large_p$p, z = large_p$models) %>% add_surface()
large_p
plot_ly(x = large_p[,1], y = large_p[,2], z = large_p[,3]) %>% add_surface()
plot_ly(x = large_p[,1], y = large_p[,2], z = large_p) %>% add_surface()
large_p <- expand.grid(records = 5:100, p = 2:5) %>%
mutate(models = factorial(records) / (p * factorial(records - p)))
large_p <- as.matrix(large_p)
plot_ly(x = large_p[,1], y = large_p[,2], z = large_p) %>% add_surface()
large_p <- expand.grid(records = 5:100, p = 2:5) %>%
mutate(models = factorial(records) / (p * factorial(records - p)),
p = factor(p))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = models, color = p)) +
labs(x = '# of Observations in Original Data Set',
y = '# of Models Generated for Cross-Validation',
title = 'Cross-Validation Models Generated per Data Set Size')
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = '# of Models Generated for Cross-Validation',
title = 'Cross-Validation Models Generated per Data Set Size')
exhaustive_cv %>%
ggplot() +
geom_line(aes(x = records, y = models)) +
labs(x = '# of Observations in Original Data Set',
y = '# of Models Generated for Cross-Validation',
title = 'Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = 2')
large_p <- expand.grid(records = 5:100, p = 2:5) %>%
mutate(models = factorial(records) / (p * factorial(records - p)),
p = factor(p))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p))
labs(x = 'log of the # of Observations in Original Data Set',
y = '# of Models Generated for Cross-Validation',
title = 'Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]')
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = 'log of the # of Observations in Original Data Set',
y = '# of Models Generated for Cross-Validation',
title = 'Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]')
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(labels = scales::number_format(big.mark = ','))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]')
max(large_p$models)
log(max(large_p$models))
exp(log(max(large_p$models)))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(7))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(10))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(9))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks())
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(8))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(7))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(8))
scales::pretty_breaks(8)
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log of the # of Models Generated for Cross-Validation',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(8))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log(# of Models Generated for Cross-Validation)',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(8))
library(reticulate)
library(tidyverse)
library(plotly)
library(tidymodels)
use_python('C:/ProgramData/Anaconda3/python.exe', T)
set.seed(279)
theme_set(theme_bw())
data(iris)
iris <- as_tibble(iris)
head(iris)
dim(iris)
iris_split <- initial_split(iris, prob = 0.8)
iris_train <- training(iris_split)
iris_test  <- testing(iris_split)
iris_train
err <- c()
# for each observation in our data set
for (i in 1:nrow(iris_train)) {
# fit a model leaving that observation out
lm <- lm(Petal.Length ~ Sepal.Length, data = iris_train[-i,])
# then store the squared residual
err[i] <- (iris_train$Petal.Length[i] - predict(lm, newdata = iris_train[i,]))**2
}
# report the average squared residual
print(round(mean(err), 4))
iris_train
lm_model <- linear_reg() %>%
set_engine('lm')
iris_folds <- vfold_cv(iris_train, v = 10)
lm_fit <- lm_model %>%
fit_resamples(Petal.Length ~ Sepal.Length, resamples = iris_folds, metrics = 'RMSE')
lm_fit <- lm_model %>%
fit_resamples(Petal.Length ~ Sepal.Length, resamples = iris_folds)
lm_fit
lm_fit$.metrics
lm_fit$.notes
lm_fit$splits
head(lm_fit)
lm_fit %>% pluck('fit')
lm_fit %>% pluck('.metrics')
lm_fit %>% pluck('.metrics')[[1]]
lm_fit %>% pluck('.metrics')[1]
lm_fit %>% pluck('.metrics')
lm_fit %>%
mutate(RMSE = map(.metrics, function(x) {x %>% pull(RMSE)}))
head(lm_fit$.metrics)
lm_fit %>%
mutate(RMSE = map(.metrics, function(x) {x %>% filter(.metric == 'rmse') %>% pull(.estimate)}))
lm_fit %>%
mutate(RMSE = unlist(map(.metrics, function(x) {x %>% filter(.metric == 'rmse') %>% pull(.estimate)})))
lm_fit %>%
mutate(RMSE =
unlist(
map(.metrics,
function(x) {
x %>%
filter(.metric == 'rmse') %>%
pull(.estimate)
}
)
)
) %>%
summarise(avg_RMSE = mean(RMSE))
head(lm_fit)
lm_fit
library(reticulate)
library(tidyverse)
library(plotly)
library(tidymodels)
use_python('C:/ProgramData/Anaconda3/python.exe', T)
library(reticulate)
library(tidyverse)
library(plotly)
library(tidymodels)
use_python('C:/ProgramData/Anaconda3/python.exe', T)
set.seed(279)
theme_set(theme_bw())
data(iris)
iris <- as_tibble(iris)
lm_model <- linear_reg() %>%
set_engine('lm')
head(iris)
dim(iris)
iris_split <- initial_split(iris, prob = 0.8)
iris_train <- training(iris_split)
iris_test  <- testing(iris_split)
err <- c()
# for each observation in our data set
for (i in 1:nrow(iris_train)) {
# fit a model leaving that observation out
lm <- lm(Petal.Length ~ Sepal.Length, data = iris_train[-i,])
# then store the squared residual
err[i] <- (iris_train$Petal.Length[i] - predict(lm, newdata = iris_train[i,]))**2
}
# report the average squared residual
print(round(mean(err), 4))
library(reticulate)
library(tidyverse)
library(plotly)
library(tidymodels)
use_python('C:/ProgramData/Anaconda3/python.exe', T)
set.seed(279)
theme_set(theme_bw())
data(iris)
iris <- as_tibble(iris)
lm_model <- linear_reg() %>%
set_engine('lm')
head(iris)
dim(iris)
iris_split <- initial_split(iris, prob = 0.8)
iris_train <- training(iris_split)
iris_test  <- testing(iris_split)
err <- c()
# for each observation in our data set
for (i in 1:nrow(iris_train)) {
# fit a model leaving that observation out
lm <- lm(Petal.Length ~ Sepal.Length, data = iris_train[-i,])
# then store the squared residual
err[i] <- (iris_train$Petal.Length[i] - predict(lm, newdata = iris_train[i,]))**2
}
# report the average squared residual
print(round(mean(err), 4))
some_data <- data.frame(predictor = 1:5, response = LETTERS[1:5])
print(some_data)
for (i in 1:nrow(some_data)) {
for (j in 2:nrow(some_data))
if (i < j & sum(i,j) < 7) {
print(some_data[c(i,j),])
}
}
exhaustive_cv <- data.frame(records = 5:100) %>%
mutate(models = factorial(records) / (2 * factorial(records - 2)))
exhaustive_cv %>%
ggplot() +
geom_line(aes(x = records, y = models)) +
labs(x = '# of Observations in Original Data Set',
y = '# of Models Generated for Cross-Validation',
title = 'Leave p Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = 2')
large_p <- expand.grid(records = 5:100, p = 2:5) %>%
mutate(models = factorial(records) / (p * factorial(records - p)),
p = factor(p))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log(# of Models Generated for Cross-Validation)',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(8))
iris_folds <- vfold_cv(iris_train, v = 10)
lm_fit <- lm_model %>%
fit_resamples(Petal.Length ~ Sepal.Length, resamples = iris_folds)
lm_fit
lm_fit %>%
mutate(RMSE =
unlist(
map(.metrics,
function(x) {
x %>%
filter(.metric == 'rmse') %>%
pull(.estimate)
}
)
)
) %>%
summarise(avg_RMSE = mean(RMSE))
lm_fit %>%
mutate(RMSE =
unlist(
map(.metrics,
function(x) {
x %>%
filter(.metric == 'rmse') %>%
pull(.estimate)
}
)
)
)
lm_fit %>%
mutate(RMSE =
unlist(
map(.metrics,
function(x) {
x %>%
filter(.metric == 'rmse') %>%
pull(.estimate)
}
)
)
) %>%
summarise(avg_RMSE = mean(RMSE))
lm_fit %>%
mutate(RMSE =
unlist(
map(.metrics,
function(x) {
x %>%
filter(.metric == 'rmse') %>%
pull(.estimate)
}
)
)
) %>%
summarise(avg_RMSE = round(mean(RMSE), 4))
library(reticulate)
library(tidyverse)
library(plotly)
library(tidymodels)
use_python('C:/ProgramData/Anaconda3/python.exe', T)
set.seed(279)
theme_set(theme_bw())
data(iris)
iris <- as_tibble(iris)
lm_model <- linear_reg() %>%
set_engine('lm')
head(iris)
dim(iris)
iris_split <- initial_split(iris, prob = 0.8)
iris_train <- training(iris_split)
iris_test  <- testing(iris_split)
dim(iris_train)
dim(iris_test)
err <- c()
# for each observation in our data set
for (i in 1:nrow(iris_train)) {
# fit a model leaving that observation out
lm <- lm(Petal.Length ~ Sepal.Length, data = iris_train[-i,])
# then store the squared residual
err[i] <- (iris_train$Petal.Length[i] - predict(lm, newdata = iris_train[i,]))**2
}
# report the average squared residual
print(round(mean(err), 4))
some_data <- data.frame(predictor = 1:5, response = LETTERS[1:5])
print(some_data)
for (i in 1:nrow(some_data)) {
for (j in 2:nrow(some_data))
if (i < j & sum(i,j) < 7) {
print(some_data[c(i,j),])
}
}
exhaustive_cv <- data.frame(records = 5:100) %>%
mutate(models = factorial(records) / (2 * factorial(records - 2)))
exhaustive_cv %>%
ggplot() +
geom_line(aes(x = records, y = models)) +
labs(x = '# of Observations in Original Data Set',
y = '# of Models Generated for Cross-Validation',
title = 'Leave p Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = 2')
large_p <- expand.grid(records = 5:100, p = 2:5) %>%
mutate(models = factorial(records) / (p * factorial(records - p)),
p = factor(p))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p)) +
labs(x = '# of Observations in Original Data Set',
y = 'log(# of Models Generated for Cross-Validation)',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(8))
iris_folds <- vfold_cv(iris_train, v = 10)
lm_fit <- lm_model %>%
fit_resamples(Petal.Length ~ Sepal.Length, resamples = iris_folds)
lm_fit
lm_fit %>%
mutate(RMSE =
unlist(
map(.metrics,
function(x) {
x %>%
filter(.metric == 'rmse') %>%
pull(.estimate)
}
)
)
) %>%
summarise(avg_RMSE = round(mean(RMSE), 4))
dim(iris_test)
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p), size = 4) +
labs(x = '# of Observations in Original Data Set',
y = 'log(# of Models Generated for Cross-Validation)',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(8))
large_p %>%
ggplot() +
geom_line(aes(x = records, y = log(models), color = p), size = 1) +
labs(x = '# of Observations in Original Data Set',
y = 'log(# of Models Generated for Cross-Validation)',
title = 'Leave p Out Cross-Validation Models Generated per Data Set Size',
subtitle = 'For p = [2, 3, 4, 5]') +
scale_y_continuous(breaks = scales::pretty_breaks(8))
ggsave('cv_banner_img.png', width = 16, height = 9, units = 'in')
KnitPost <- function(input, outfile, base.url="/") {
# this function is a modified version of an example here:
# https://github.com/ouzor/ouzor.github.com/blob/master/_knitposts.R
require(knitr);
opts_knit$set(base.url = base.url)
fig.path <- paste0("blog/addons/", Sys.Date(), '-', sub(".Rmd$", "", basename(input)), "/")
opts_chunk$set(fig.path = fig.path)
opts_chunk$set(fig.cap = "testing")
render_jekyll()
knit(input, outfile, envir = parent.frame())
}
for (infile in list.files("blog/_posts/drafts/RMarkDowns", pattern="*.Rmd", full.names=TRUE)) {
outfile = paste0("blog/_posts/drafts/Staging/", sub(".Rmd$", ".md", basename(infile)))
# knit only if the input file is the last one modified
if (!file.exists(outfile) |
file.info(infile)$mtime > file.info(outfile)$mtime) {
KnitPost(infile, outfile)
}
}
for (infile in list.files("blog/_posts/drafts/RMarkDowns", pattern="*.Rmd", full.names=TRUE)) {
outfile = paste0("blog/_posts/drafts/Staging/", sub(".Rmd$", ".md", basename(infile)))
# knit only if the input file is the last one modified
if (!file.exists(outfile) |
file.info(infile)$mtime > file.info(outfile)$mtime) {
KnitPost(infile, outfile)
}
}
list.files("blog/_posts/drafts/RMarkDowns", pattern="*.Rmd", full.names=TRUE)
getwd()
setwd("~/Projects/Computer Science Projects/Data Scientist Portfolio/9Olive.github.io")
for (infile in list.files("blog/_posts/drafts/RMarkDowns", pattern="*.Rmd", full.names=TRUE)) {
outfile = paste0("blog/_posts/drafts/Staging/", sub(".Rmd$", ".md", basename(infile)))
# knit only if the input file is the last one modified
if (!file.exists(outfile) |
file.info(infile)$mtime > file.info(outfile)$mtime) {
KnitPost(infile, outfile)
}
}
for (infile in list.files("blog/_posts/drafts/RMarkDowns", pattern="*.Rmd", full.names=TRUE)) {
outfile = paste0("blog/_posts/drafts/Staging/", sub(".Rmd$", ".md", basename(infile)))
# knit only if the input file is the last one modified
if (!file.exists(outfile) |
file.info(infile)$mtime > file.info(outfile)$mtime) {
KnitPost(infile, outfile)
}
}
